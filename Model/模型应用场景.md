[结构](https://static.coggle.it/diagram/WHeBqDIrJRk-kDDY/t/categories-of-algorithms-non-exhaustive)

![title](https://raw.githubusercontent.com/azusakou/figures_study_ML/main/Users/2021/04/22/structure-1619067503112.jpg)

![title](https://raw.githubusercontent.com/azusakou/figures_study_ML/main/Users/2021/04/22/v2-4e44623163b15cd12c8e77ef286ebafc_r-1619067577486.jpg)

[不同数据集上（121个），不同的分类器（179个）的实际效果](https://github.com/azusakou/studynote_ML/blob/master/Model/Do%20we%20Need%20Hundreds%20of%20Classifiers%20to%20Solve%20Real%20World%20Classification%20Problems%3F.pdf)
随机森林平均来说最强，但也只在9.9%的数据集上拿到了第一，优点是鲜有短板。  
SVM的平均水平紧随其后，在10.7%的数据集上拿到第一。  
神经网络（13.2%）和boosting（~9%）表现不错。  
数据维度越高，随机森林就比AdaBoost强越多，但是整体不及SVM[2]。  
数据量越大，神经网络就越强。  






# Nearest Neighbor

典型的例子是KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。它的特点是完全跟着数据走，没有数学模型可言。

适用情景：
需要一个特别容易解释的模型的时候。比如需要向用户解释原因的推荐算法。

# Bayesian

典型的例子是Naive Bayes，核心思路是根据条件概率计算待判断点的类型。是相对容易理解的一个模型，至今依然被垃圾邮件过滤器使用。

适用情景：
需要一个比较容易解释，而且不同维度之间相关性较小的模型的时候。可以高效处理高维数据，虽然结果可能不尽如人意。

# Decision tree

决策树的特点是它总是在沿着特征做切分。随着层层递进，这个划分会越来越细。虽然生成的树不容易给用户看，但是数据分析的时候，通过观察树的上层结构，能够对分类器的核心思路有一个直观的感受。举个简单的例子，当我们预测一个孩子的身高的时候，决策树的第一层可能是这个孩子的性别。男生走左边的树进行进一步预测，女生则走右边的树。这就说明性别对身高有很强的影响。

适用情景：
因为它能够生成清晰的基于特征(feature)选择不同预测结果的树状结构，数据分析师希望更好的理解手上的数据的时候往往可以使用决策树。同时它也是相对容易被攻击的分类器[3]。这里的攻击是指人为的改变一些特征，使得分类器判断错误。常见于垃圾邮件躲避检测中。因为决策树最终在底层判断是基于单个条件的，攻击者往往只需要改变很少的特征就可以逃过监测。受限于它的简单性，决策树更大的用处是作为一些更有用的算法的基石。

# Random forest

提到决策树就不得不提随机森林。顾名思义，森林就是很多树。严格来说，随机森林其实算是一种集成算法。它首先随机选取不同的特征(feature)和训练样本(training sample)，生成大量的决策树，然后综合这些决策树的结果来进行最终的分类。随机森林在现实分析中被大量使用，它相对于决策树，在准确性上有了很大的提升，同时一定程度上改善了决策树容易被攻击的特点。

适用情景：
数据维度相对低（几十维），同时对准确性有较高要求时。因为不需要很多参数调整就可以达到不错的效果，基本上不知道用什么方法的时候都可以先试一下随机森林。

# SVM (Support vector machine)

SVM的核心思想就是找到不同类别之间的分界面，使得两类样本尽量落在面的两边，而且离分界面尽量远。最早的SVM是平面的，局限很大。但是利用核函数(kernel function)，我们可以把平面投射(mapping)成曲面，进而大大提高SVM的适用范围。




