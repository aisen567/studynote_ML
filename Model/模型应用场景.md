[结构](https://static.coggle.it/diagram/WHeBqDIrJRk-kDDY/t/categories-of-algorithms-non-exhaustive)

![title](https://raw.githubusercontent.com/azusakou/figures_study_ML/main/Users/2021/04/22/structure-1619067503112.jpg)

![title](https://raw.githubusercontent.com/azusakou/figures_study_ML/main/Users/2021/04/22/v2-4e44623163b15cd12c8e77ef286ebafc_r-1619067577486.jpg)

[不同数据集上（121个），不同的分类器（179个）的实际效果](https://github.com/azusakou/studynote_ML/blob/master/Model/Do%20we%20Need%20Hundreds%20of%20Classifiers%20to%20Solve%20Real%20World%20Classification%20Problems%3F.pdf)
随机森林平均来说最强，但也只在9.9%的数据集上拿到了第一，优点是鲜有短板。  
SVM的平均水平紧随其后，在10.7%的数据集上拿到第一。  
神经网络（13.2%）和boosting（~9%）表现不错。  
数据维度越高，随机森林就比AdaBoost强越多，但是整体不及SVM[2]。  
数据量越大，神经网络就越强。  






# Nearest Neighbor

典型的例子是KNN，它的思路就是——对于待判断的点，找到离它最近的几个数据点，根据它们的类型决定待判断点的类型。它的特点是完全跟着数据走，没有数学模型可言。

适用情景：
需要一个特别容易解释的模型的时候。比如需要向用户解释原因的推荐算法。

# Bayesian

典型的例子是Naive Bayes，核心思路是根据条件概率计算待判断点的类型。是相对容易理解的一个模型，至今依然被垃圾邮件过滤器使用。

适用情景：

需要一个比较容易解释，而且不同维度之间相关性较小的模型的时候。

可以高效处理高维数据，虽然结果可能不尽如人意。








