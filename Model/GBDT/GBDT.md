
# 梯度提升树
[]()

## 知识点
通俗易懂理解——Adaboost算法原理  
[AdaBoost](https://zhuanlan.zhihu.com/p/41536315)（Adaptive Boosting，自适应增强），其自适应在于：**前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。**  

Adaboost 迭代算法有三步：  
1. 初始化训练样本的权值分布，每个样本具有相同权重；  
2. 训练弱分类器，如果样本分类正确，则在构造下一个训练集中，它的权值就会被降低；反之提高。用更新过的样本集去训练下一个分类器；  
3. 将所有弱分类组合成强分类器，各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，降低分类误差率大的弱分类器的权重。  

[拓展阅读:Random Forest、Adaboost、GBDT](https://zhuanlan.zhihu.com/p/86263786)  
[XGBoost、LightGBM原理](https://zhuanlan.zhihu.com/p/87885678)  
[XGBoost、LightGBM对比](https://zhuanlan.zhihu.com/p/35645973)  

## QA
### 1.介绍一下[GBDT](https://zhuanlan.zhihu.com/p/29765582)
GBDT是boost结构的非线性模型，首先初始化回归树，然后对每个样本计算残差（损失函数为均方差时，残差为负梯度方向），将得到的残差作为样本训练下一颗回归树，通过贪心策略生成决策树的每个节点，通过迭代求得一个新的树模型，使整体损失函数变小，通过调整树模型的学习率抑制模型过拟合。GBDT的求解过程就是梯度下降在函数空间的优化过程。优点是能处理连续和离散数据，易于特征选择，缺点是串行生成，对于稀疏特征效果不如LR或SVM。

### 2.xgboost有哪些改进？ 
xgb将目标函数泰勒展开到二阶，保留里更多有关目标函数的信息。  
xgb加入了权重和叶子数的l2正则项，提高泛化能力。  
xgb支持多种类型的分类器。  
xgb采用了类似RF的策略，可以对样本和特征进行采样。  
xgb通过稀疏感知算法处理缺失值。  
xgb对特征进行排序，分裂节点时并行查找每个特征的最佳分割点。  

### 3.GBDT与随机森林的异同点？ 
相同点都是由caret树组成的融合模型。  
不同点：RF是bagging结构，GBDT是boosting结构。  
RF中各个caret树并行，所以多个模型的结果能降低方差，GBDT串行，不断降低偏差。  
RF采用bootstrap采样，GBDT采用全部样本。  
RF结果采用平均或多数表决，GBDT采用加权融合。  
GBDT更容易过拟合。  

### 4.xgb防止过拟合有什么方法，如何调参？ 
eta[0,1]: 更小的值能降低每个树的权重  
gamma[0,∞]：预剪枝，最小分裂损失值，节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守  
max_depth[0,∞]: 树的深度，越深越容易过拟合  
min_child_weight[0,∞]：最小叶子节点样本权重和，值越大越能避免过拟合，但是容易导致欠拟合，需要用交叉验证来调整  
subsample (0,1]：随机采样的比例，减小该值能避免过拟合  
colsample_bytree (0,1]：对特征采样的比例，减小该值能避免过拟合  
lambda [ reg_lambda]：权重的L2正则化项  
alpha [ reg_alpha]：权重的L1正则化项  
scale_pos_weight：样本比例非常不均衡时，可以将其设置为负样本的数目与正样本数目的比值  

### 5.xgb为什么对缺失值不敏感，如何处理缺失值的？ 
采用稀疏感知算法， 在某列特征上寻找分裂节点时，先对非缺失样本上对特征值进行遍历，对缺失值样本，分别分配到两个节点，然后选择分裂后增益最大对方向，作为预测时特征缺失样本对默认方向。如果训练集中没有缺失值，但是测试集中有，那么默认将缺失值划分到右叶子节点方向。

### 6.解释一下GBDT沿着梯度下降方向提升，如何实现的？
在参数空间优化，每次迭代得到的参数的增量是负梯度乘以学习率，在函数空间优化，每次得到增量函数（在GBDT中就是各个决策树），这个函数会拟合负梯度，最终的结果，是初始值加上每次的增量。只要损失函数一阶可导，每个决策树拟合的都是负梯度，不是用负梯度代替残差，而是当损失函数是均方差损失时，负梯度是残差。

