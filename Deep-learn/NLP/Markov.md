# 1 马尔科夫链（维基百科）

马尔可夫链（英语：Markov chain），又称离散时间马尔可夫链（discrete-time Markov chain，缩写为DTMC），因俄国数学家安德烈·马尔可夫得名，为状态空间中经过从一个状态到另一个状态的转换的随机过程。该过程要求具备“无记忆”的性质：下一状态的概率分布只能由当前状态决定，在时间序列中它前面的事件均与之无关。这种特定类型的“无记忆性”称作马尔可夫性质。

 

# 2 马尔科夫过程——离散的叫马尔科夫链

在概率论及统计学中，马尔可夫过程（英语：Markov process）是一个具备了马尔可夫性质的随机过程，因为俄国数学家安德雷·马尔可夫得名。马尔可夫过程是不具备记忆特质的（memorylessness）。换言之，马尔可夫过程的条件概率仅仅与系统的当前状态相关，而与它的过去历史或未来状态，都是独立、不相关的。

具备离散状态的马尔可夫过程，通常被称为马尔可夫链。马尔可夫链通常使用离散的时间集合定义，又称离散时间马尔可夫链。有些学者虽然采用这个术语，但允许时间可以取连续的值。

 

# 3 隐马尔科夫模型定义

隐马尔可夫模型（Hidden Markov Model；缩写：HMM）或称作隐性马尔可夫模型，是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。
在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中，状态并不是直接可见的，但受状态影响的某些变量则是可见的。每一个状态在可能输出的符号上都有一概率分布。因此输出符号的序列能够透露出状态序列的一些信息。
隐马尔科夫模型是关于时序的概率模型，描述一个由隐藏的马尔科夫链随机生成不可观测的状态随机序列，再由各个状态生成一个可观测的观测随机序列的过程。

下面通过一个村民看病的故事理解什么是HMM模型。  
想象一个乡村诊所，村民的身体状况要么健康要么发烧，他们只有问诊所的医生才能知道是否发烧。  
医生通过询问村民的感觉去诊断他们是否发烧。村民自身的感觉有正常、头晕或冷。  
假设一个村民每天来到诊所并告诉医生他的感觉。村民的感觉只由他当天的健康状况决定。  
村民的健康状态有两种：健康和发烧，但医生不能直接观察到，这意味着健康状态对医生是不可见的。  
每天村民会告诉医生自己有以下几种由他的健康状态决定的感觉的一种：正常、冷或头晕。  
于是医生会得到一个村民的感觉的观测序列，例如这样：{正常，冷，冷，头晕，冷，头晕，冷，正常，正常}。  
但是村民的健康状态这个序列是需要由医生根据模型来推断的，是不可直接观测的。  
这个村民看病的故事中由村民的健康状态序列和村民的感觉序列构成的系统就是一个隐马尔科夫模型(HMM)。  
其中村民的健康状态序列构成一个马尔科夫链。其每个序列值只和前一个值有关，和其它值无关。由于这个马尔科夫链是隐藏的，不可以被直接观测到，只能由其关联的村民的感觉序列来进行推断，因此叫做隐马尔科夫模型(HMM)。

## 3.1 HMM模型的上帝视角
HMM模型是一个生成模型，描述了两个相关序列的依赖关系。这两个相关序列称为状态序列[X<sub>1</sub>, X<sub>2</sub>, X<sub>3</sub>,..., X<sub>T</sub>] 和 观测序列 [O<sub>1</sub>, O<sub>2</sub>, O<sub>3</sub>,..., O<sub>T</sub>].其中状态序列在t时刻的值只和t-1时刻状态序列的取值有关，观测序列在t时刻的值只和t时刻观测序列的取值有关。
其联合概率函数如下：
![title](https://raw.githubusercontent.com/azusakou/figures_study_ML/main/Users/2021/05/17/1621237116070-1621237325788.jpg)

## 3.2 HMM的三大假设
1，马尔科夫性假设：t时刻的状态出现的概率只和t-1时刻的状态有关。
![title](https://raw.githubusercontent.com/azusakou/figures_study_ML/main/Users/2021/05/17/equation-1621237410819.svg)
2，齐次性假设：可以理解为时间平移不变性

3，观测独立性假设：某个时刻t的观测值只依赖于该时刻的状态值，与任何其它时刻的观测值和状态值无关。

上述HMM的联合概率函数中，实际上就用到了HMM的三大假设。


# 4 隐马尔科夫模型参数的确定

## 4.1 初始概率分布\pi

 i_1可能是状态1，状态2 ... 状态n，于是i_1就有个N点分布：

|i<sub>1</sub>|state 1|state 2|...|state n|
|----|----|----|----|----|
|*P*|P<sub>1</sub>|p<sub>2</sub>|...|p<sub>n</sub>|

即：i_1对应个n维的向量。
上面这个n维的向量就是初始概率分布，记做π。  

## 4.2 状态转移矩阵A
因为i_2和i_1不独立，所以i_2是状态1的概率有：i_1是状态1时i_2是状态1，i_1是状态2时i_2是状态1,..., i_1是状态n时i_2是状态1，如下表

|i<sub>2</sub>/i<sub>1</sub>|state 1|state 2|...|state n|
|----|----|----|----|----|
|state 1|P<sub>11</sub>|p<sub>12</sub>|...|p<sub>1n</sub>|
|state 2|P<sub>21</sub>|p<sub>22</sub>|...|p<sub>2n</sub>|
|...|...|...|...|...|
|state n|P<sub>n1</sub>|p<sub>n2</sub>|...|p<sub>nn</sub>|

即：i_1->i_2对应n*n的矩阵。  
同理：i_n -> i_{n+1}对应个n*n的矩阵。  
上面这些n*n的矩阵被称为状态转移矩阵，用An*n表示。  
当然了，真要说的话，i_n -> i_{n+1}的状态转移矩阵一定都不一样，但在实际应用中一般将这些状态转移矩阵定为同一个，即：只有一个状态转移矩阵。  

## 4.3 观测矩阵B

如果对于i_n有：状态1, 状态2, ..., 状态n，那i_n的每一个状态都会从下面的m个观测中产生一个：观测1, 观测2, ..., 观测m，所以有如下矩阵：

|i<sub>n</sub>/O<sub>m</sub>|Obs 1|Obs 2|...|Obs m|
|----|----|----|----|----|
|state 1|P<sub>11</sub>|p<sub>12</sub>|...|p<sub>1m</sub>|
|state 2|P<sub>21</sub>|p<sub>22</sub>|...|p<sub>2m</sub>|
|...|...|...|...|...|
|state n|P<sub>n1</sub>|p<sub>n2</sub>|...|p<sub>nm</sub>|

这可以用一个n*m的矩阵表示，也就是观测矩阵，记做Bn*m。

由于HMM用上面的π，A，B就可以描述了，于是我们就可以说：HMM由初始概率分布π、状态转移概率分布A以及观测概率分布B确定，为了方便表达，把A, B, π 用 λ 表示，即：

            λ = (A, B, π)

