![title](https://raw.githubusercontent.com/azusakou/figures_study_ML/main/Users/2021/04/22/LR-1619061114290.tif)
回归分析是一种预测性的建模技术,它研究的是因变量(目标)和自变量(特征)之间的关系,用于预测分析。

## 学习目标:  
理解最大似然估计及线性回归MSE损失函数的推导过程;  
理解通过梯度下降优化方法[原理及代码](https://zhuanlan.zhihu.com/p/36564434),更新线性回归模型参数过程;  
能在真实数据中正确应用线性回归解决问题;  

## 知识点：  
1.最小二乘:基于经验的最小化误差平方根MSE; 误差服从正态分布是最小二乘的充要条件,MSE损失函数的推导;  
2.最大似然估计  
3.Gradient Descent  
4.[参数估计/假设检验/拟合优度](https://zhuanlan.zhihu.com/p/48541799)  
5.[linear reg/ridge reg/Bayesian reg](https://zhuanlan.zhihu.com/p/86009986)

## Q&A
### 1.如何将类别型变量引入线性回归，提出至少1种方案？ 
Dummy encoding, one-hot encoding

### 2.自变量进行标准化会对模型带来哪些影响（尝试从梯度、回归系数等角度）？尝试在项目过程中进行实验。
梯度：消除特征之间的量纲影响，加快收敛速度 
回归系数：连续型变量的回归系数在标准化之后变大，分类变量的回归系数在标准化之后变小

### 3.因变量正态分布 误差正态分布

线性回归满足的前提是line，一是自变量和因变量之间存在线性关系l，各观测之间相互独立i，而这里的满足正态分布是残差e服从正态分布，并不是因变量满足正态分布， e指的是残差满足方差齐性。在样本量足够大的时候，可以不去考虑正态分布的问题；如果样本量不够可以通过观察残差分析，观察残差分析图来判断方差的齐性。