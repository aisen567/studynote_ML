# 决策树

## 熵
熵：表示随机变量的不确定性。  
[信息熵](https://zhuanlan.zhihu.com/p/89958871)：一个表征符号系统中单位符号平均信息量的指标。  
[条件熵](https://zhuanlan.zhihu.com/p/26551798)：在一个条件下，随机变量的不确定性。  
[信息增益](https://zhuanlan.zhihu.com/p/26596036)：熵 - 条件熵。表示在一个条件下，信息不确定性减少的程度。  

*通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量，在阴天情况下下雨的信息
熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。
X的熵减去Y条件下X的熵，就是信息增益。具体解释：原本明天下雨的信息熵是2，条件熵是0.01（因为如果知道明
天是阴天，那么下雨的概率很大，信息量少），这样相减后为1.99。在获得阴天这个信息后，下雨信息不确定性减少
了1.99，不确定减少了很多，所以信息增益大。也就是说，阴天这个信息对明天下午这一推断来说非常重要。*

## DT
所以在特征选择的时候常常用信息增益，如果IG（信息增益大）的话那么这个特征对于分类来说很关键，决策树就是
这样来找特征的。  

[]()
[决策树](https://zhuanlan.zhihu.com/p/26703300)  
[基于信息与信息增益的ID3及C4.5决策树](https://www.cnblogs.com/pinard/p/6050306.html)  
[基尼指数（基尼不纯度,信息熵的1阶泰勒展开）](https://www.zhihu.com/question/296781126/answer/508112100)  
[CART树](https://www.cnblogs.com/pinard/p/6053344.html)  

## Ensemble learning
[bagging模型集成与随机森岭](https://www.cnblogs.com/pinard/p/6156009.html)  
[模型融合](https://zhuanlan.zhihu.com/p/25836678)  
[预测偏差、方差与模型融合](https://github.com/azusakou/studynote_ML/blob/master/DT/Lecture_10_Ensemble.pdf)  

## QA
1.采用信息增益、信息增益率作为决策树生长策略，有什么区别?  
用信息增益作为标准容易偏向于取值较多的特征，信息增益率是信息增益和特征熵的比值，可以校正信息增益容易偏向于取值较多的特征的问题。

2.其他条件一致，对样本某变量进行单调非线性变化，是否会影响决策树生长，为什么?  
不会影响。因为决策树的生长基于概率，数值变化对概率分布没有影响。

3.随机森林参数有哪些重要的[参数](https://zhuanlan.zhihu.com/p/56940098) ，分别的作用试什么?  
n_estimators:对原始数据集进行有放回抽样生成的子数据集个数，即决策树的个数  
max_features:构建决策树最优模型时考虑的最大特征数。  
max_depth:决策树最大深度  
min_samples_split：内部节点再划分所需最小样本数  
min_samples_leaf:叶子节点含有的最少样本  
4.多个模型预测结果做Average融合，模型间具备怎样的特点会取得更好的效果?  
预测误差之间不相关，在表现良好的情况下，使用更多的模型







