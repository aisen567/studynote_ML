# 决策树[]()

## 熵
熵：表示随机变量的不确定性。  
[信息熵](https://zhuanlan.zhihu.com/p/89958871)：一个表征符号系统中单位符号平均信息量的指标。  
[条件熵](https://zhuanlan.zhihu.com/p/26551798)：在一个条件下，随机变量的不确定性。  
[信息增益](https://zhuanlan.zhihu.com/p/26596036)：熵 - 条件熵。表示在一个条件下，信息不确定性减少的程度。  

*通俗地讲，X(明天下雨)是一个随机变量，X的熵可以算出来， Y(明天阴天)也是随机变量，在阴天情况下下雨的信息
熵我们如果也知道的话（此处需要知道其联合概率分布或是通过数据估计）即是条件熵。
X的熵减去Y条件下X的熵，就是信息增益。具体解释：原本明天下雨的信息熵是2，条件熵是0.01（因为如果知道明
天是阴天，那么下雨的概率很大，信息量少），这样相减后为1.99。在获得阴天这个信息后，下雨信息不确定性减少
了1.99，不确定减少了很多，所以信息增益大。也就是说，阴天这个信息对明天下午这一推断来说非常重要。*

## DT
所以在特征选择的时候常常用信息增益，如果IG（信息增益大）的话那么这个特征对于分类来说很关键，[决策树](https://zhuanlan.zhihu.com/p/26703300) 就是这样来找特征的。  

[基于信息与信息增益的ID3及C4.5决策树](https://www.cnblogs.com/pinard/p/6050306.html)  
[基尼指数（基尼不纯度,信息熵的1阶泰勒展开）](https://www.zhihu.com/question/296781126/answer/508112100)  
[CART树](https://www.cnblogs.com/pinard/p/6053344.html)  

## Ensemble learning
## 回顾：随机森林
[Random Forest（随机森林）是 Bagging 的扩展变体](https://www.cnblogs.com/pinard/p/6156009.html)，它在以决策树为基学习器构建 Bagging 集成的基础上，进一步在决策树的训练过程中引入了随机特征选择，因此可以概括 RF 包括四个部分：  
1. 随机选择样本（放回抽样）；  
2. 随机选择特征；  
3. 构建决策树；  
4. 随机森林投票（平均）。  
随机选择样本和 Bagging 相同，采用的是 Bootstrap 自助采样法；**随机选择特征是指在每个节点在分裂过程中都是随机选择特征的**（区别与每棵树随机选择一批特征）。  
这种随机性导致随机森林的偏差会有稍微的增加（相比于单棵不随机树），但是由于随机森林的“平均”特性，会使得它的方差减小，而且方差的减小补偿了偏差的增大，因此总体而言是更好的模型。
随机采样由于引入了两种采样方法保证了随机性，所以每棵树都是最大可能的进行生长就算不剪枝也不会出现过拟合。
优点
1. 在数据集上表现良好，相对于其他算法有较大的优势
2. 易于并行化，在大数据集上有很大的优势；
3. 能够处理高维度数据，不用做特征选择。

[模型融合](https://zhuanlan.zhihu.com/p/25836678)  
[预测偏差、方差与模型融合](https://github.com/azusakou/studynote_ML/blob/master/DT/Lecture_10_Ensemble.pdf)  

## QA
1.采用信息增益、信息增益率作为决策树生长策略，有什么区别?  
用信息增益作为标准容易偏向于取值较多的特征，信息增益率是信息增益和特征熵的比值，可以校正信息增益容易偏向于取值较多的特征的问题。

2.其他条件一致，对样本某变量进行单调非线性变化，是否会影响决策树生长，为什么?  
不会影响。因为决策树的生长基于概率，数值变化对概率分布没有影响。

3.随机森林参数有哪些重要的[参数](https://zhuanlan.zhihu.com/p/56940098) ，分别的作用试什么?  
n_estimators:对原始数据集进行有放回抽样生成的子数据集个数，即决策树的个数  
max_features:构建决策树最优模型时考虑的最大特征数。  
max_depth:决策树最大深度  
min_samples_split：内部节点再划分所需最小样本数  
min_samples_leaf:叶子节点含有的最少样本  
4.多个模型预测结果做Average融合，模型间具备怎样的特点会取得更好的效果?  
预测误差之间不相关，在表现良好的情况下，使用更多的模型







