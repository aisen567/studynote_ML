# 广义线性模型：逻辑回归

## 学习目标：
1. 逻辑回归损失函数推导  
2. 了解模型欠拟合、过拟合状态，了解L1、L2正则化方法减缓模型过拟合。  

## 知识点
1. [逻辑回归理解](https://zhuanlan.zhihu.com/p/44591359)  
勘误：文章损失函数部分应添加负号，对齐梯度下降方向  
2. [无偏估计](https://www.zhihu.com/question/22983179/answer/404391738)  
理解衡量一个估计好坏的三个方面：无偏（低偏差）、有效（低方差）、一致（数据划分）  
3. [方差与偏差的权衡](https://zhuanlan.zhihu.com/p/38853908)  
4. [控制方差与偏差权衡的方法：l1与l2正则化(1-7部分)](https://zhuanlan.zhihu.com/p/35356992)  
[稀疏性的图解法](https://vimsky.com/article/3852.html)  
[稀疏性的其他解释](https://zhuanlan.zhihu.com/p/50142573)  

## QA
1为什么不能直接用线性回归做分类?  
（1）稳定性差，取值范围不同。逻辑回归假设符合伯努利分布，线性回归假设误差符合正态分布，用线性回归做分类时，误差肯定不是正态分布，所以假设不一样。  
（2）线性回归增加sigmoid后，在接近极大值和极小值的时候，梯度非常小，所以对于样本的不均衡，或者一些异常样本，对这个分界的超平面影响比较小，所以适合用逻辑回归做分类。  

2. 多重共线性变量会给逻辑回归带来什么问题?为什么?怎么处理?  
（1）预测值不稳定，对特征的变化非常敏感，参数值的大小和符号可能与实际值不一样。  
（2）如果两个特征存在相关关系，一个特征改变的时候，另一个特征也会改变，无法单独考察一个特征对y的影响。  
（3）可以采用剔除相关性高的特征，降维，L2正则化对变量进行筛选，增加样本量可能会减轻多重共线性。   

3. L2正则化对

3. l1与l2正则化会给模型带来什么影响，为什么?  
l1正则化和l2正则化都可以降低模型复杂程度，减小模型方差，l1正则化能够得到稀疏的参数，l2正则化能够使参数值接近0。  

4. 逻辑回归本身只具备线性的表达能力，如何让模型学会非线性关系?  
通过对x定义一个非线性映射，增加数据的维度。  